{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class        x1        y1        z1        v1        x2        y2  \\\n",
      "0   coffee  0.606142  0.244242 -0.815652  0.999938  0.623723  0.211371   \n",
      "1   coffee  0.606009  0.244254 -0.815749  0.999938  0.623691  0.211367   \n",
      "2   coffee  0.605101  0.244437 -0.754448  0.999933  0.623574  0.211543   \n",
      "3   coffee  0.602850  0.244439 -0.760619  0.999929  0.622494  0.211495   \n",
      "4   coffee  0.602109  0.244449 -0.827007  0.999929  0.622169  0.211413   \n",
      "..     ...       ...       ...       ...       ...       ...       ...   \n",
      "63  coffee  0.595017  0.236652 -0.715213  0.999693  0.615398  0.201424   \n",
      "64  coffee  0.595277  0.236610 -0.717216  0.999694  0.615549  0.201386   \n",
      "65  coffee  0.595677  0.236591 -0.716770  0.999696  0.615762  0.201355   \n",
      "66  coffee  0.595848  0.236555 -0.703413  0.999690  0.615846  0.201329   \n",
      "67  coffee  0.596489  0.236520 -0.705800  0.999684  0.616145  0.201305   \n",
      "\n",
      "          z2        v2        x3  ...       z73  v73       x74       y74  \\\n",
      "0  -0.776338  0.999922  0.634001  ... -0.036408  0.0  0.659736  0.707858   \n",
      "1  -0.775937  0.999923  0.633704  ... -0.031901  0.0  0.660784  0.701760   \n",
      "2  -0.711242  0.999918  0.633406  ... -0.031837  0.0  0.664421  0.694444   \n",
      "3  -0.722415  0.999914  0.632134  ... -0.033219  0.0  0.666726  0.688564   \n",
      "4  -0.783732  0.999915  0.631638  ... -0.035322  0.0  0.662254  0.690637   \n",
      "..       ...       ...       ...  ...       ...  ...       ...       ...   \n",
      "63 -0.675018  0.999690  0.625393  ... -0.035599  0.0  0.639682  0.702899   \n",
      "64 -0.677513  0.999691  0.625524  ... -0.035275  0.0  0.639298  0.702949   \n",
      "65 -0.676926  0.999695  0.625710  ... -0.035735  0.0  0.639156  0.703519   \n",
      "66 -0.663141  0.999690  0.625786  ... -0.035681  0.0  0.639398  0.703472   \n",
      "67 -0.665418  0.999685  0.626092  ... -0.035489  0.0  0.639886  0.703005   \n",
      "\n",
      "         z74  v74       x75       y75       z75  v75  \n",
      "0  -0.029399  0.0  0.671787  0.705231 -0.024129  0.0  \n",
      "1  -0.027481  0.0  0.672509  0.700027 -0.023272  0.0  \n",
      "2  -0.027150  0.0  0.676313  0.692712 -0.022846  0.0  \n",
      "3  -0.028706  0.0  0.678016  0.686593 -0.024647  0.0  \n",
      "4  -0.030620  0.0  0.674081  0.689211 -0.026338  0.0  \n",
      "..       ...  ...       ...       ...       ...  ...  \n",
      "63 -0.030827  0.0  0.651834  0.700998 -0.026451  0.0  \n",
      "64 -0.030290  0.0  0.651430  0.701011 -0.025782  0.0  \n",
      "65 -0.031031  0.0  0.651152  0.701208 -0.026663  0.0  \n",
      "66 -0.030801  0.0  0.651386  0.701435 -0.026377  0.0  \n",
      "67 -0.030499  0.0  0.651725  0.700917 -0.026024  0.0  \n",
      "\n",
      "[68 rows x 301 columns]\n"
     ]
    }
   ],
   "source": [
    "#import the data ---> these lines are just testing.\n",
    "# test = pd.read_csv(\"../Database-augmented/coffee/_wijo648v0g3208.csv\")\n",
    "test = pd.read_csv(\"original_data_ASL/data_four_labels/coffee/ax2UGtA8h3E2137.csv\")\n",
    "print(test)\n",
    "\n",
    "# test2=pd.read_csv(\"data_four_labels/coffee/2sGQuduhAf41354.csv\")\n",
    "# print(test2)\n",
    "\n",
    "# final_df=pd.concat([test,test2])\n",
    "# print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 36\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# train_df = pd.DataFrame()\n",
    "# test_df = pd.DataFrame()\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test =[]\n",
    "\n",
    "names = [\"coffee\", \"dog\", \"door\", \"milk\"]\n",
    "labels_dict = {\"coffee\": 0, \"dog\": 1, \"door\": 2, \"milk\": 3}\n",
    "i = 0\n",
    "for label in names:\n",
    "    # files = glob.glob(f\"data_four_labels/{label}/*.csv\")\n",
    "    files = glob.glob(f\"original_data_ASL/data_four_labels/{label}/*.csv\")\n",
    "    for f in files:\n",
    "        csv = pd.read_csv(f)\n",
    "        label = [labels_dict[csv[\"class\"].iloc[0]]]\n",
    "        csv = csv.drop(columns=[\"class\"]).to_numpy()\n",
    "        csv = csv.reshape((len(csv), 300))\n",
    "        # csv = csv.reshape((len(csv), 75, 4)) #for CNN\n",
    "\n",
    "    #this is in order to separate the training set from the test set (setting 25% of values\n",
    "        if i%4:    \n",
    "            # train_df = pd.concat([train_df, csv])\n",
    "            X_train.append(csv)\n",
    "            Y_train.append(label)\n",
    "        else:\n",
    "            # test_df = pd.concat([test_df, csv])\n",
    "            X_test.append(csv)\n",
    "            Y_test.append(label)\n",
    "        i += 1\n",
    "        \n",
    "#videos don't have the same length so we can't convert to np => keep as list\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "# print(\"training set:\")\n",
    "# print(X_train)\n",
    "# print(\"testing set: \")\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10296 3432\n"
     ]
    }
   ],
   "source": [
    "#augmented data \n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test =[]\n",
    "\n",
    "names = [\"coffee\", \"dog\", \"door\", \"milk\"]\n",
    "labels_dict = {\"coffee\": 0, \"dog\": 1, \"door\": 2, \"milk\": 3}\n",
    "i = 0\n",
    "for label in names:\n",
    "    # files = glob.glob(f\"data_four_labels/{label}/*.csv\")\n",
    "    files = glob.glob(f\"C:\\\\Users\\\\sarin\\\\Documents\\\\GitHub\\\\ASL-words-translator\\\\Augmented_data\\\\Database-augmented\\\\{label}\\\\*.csv\")\n",
    "    for f in files:\n",
    "        csv = pd.read_csv(f)\n",
    "        label = [labels_dict[csv[\"class\"].iloc[0]]]\n",
    "        csv = csv.drop(columns=[\"class\"]).to_numpy()\n",
    "        csv = csv.reshape((len(csv), 300))\n",
    "        # csv = csv.reshape((len(csv), 75, 4)) #for CNN\n",
    "\n",
    "    #this is in order to separate the training set from the test set (setting 25% of values\n",
    "        if i%4:    \n",
    "            # train_df = pd.concat([train_df, csv])\n",
    "            X_train.append(csv)\n",
    "            Y_train.append(label)\n",
    "        else:\n",
    "            # test_df = pd.concat([test_df, csv])\n",
    "            X_test.append(csv)\n",
    "            Y_test.append(label)\n",
    "        i += 1\n",
    "        \n",
    "#videos don't have the same length so we can't convert to np => keep as list\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "# print(\"training set:\")\n",
    "# print(X_train)\n",
    "# print(\"testing set: \")\n",
    "# test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since I am working on varying time frames, I will be concatenating the data into sets of n-frames together as one datapoint\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ok, use lists next time, and then transform into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header(FrameNumber, className):\n",
    "    #at first, only one element, the class name\n",
    "    headerList=[className]\n",
    "    for i in range(FrameNumber):\n",
    "        for val in range(1, num_coords+1):        \n",
    "            headerList += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]\n",
    "            all_frames = all_frames.reshape((len(df), 75, 4))\n",
    "    print(headerList)\n",
    "    return headerList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(df, FrameNumber):\n",
    "    #its the same label for each video\n",
    "    label = df[\"class\"].iloc[0]\n",
    "    # labels = df[\"class\"].values.tolist()\n",
    "    # labels = labels[FrameNumber:len(labels)]\n",
    "    return label\n",
    "\n",
    "def create_data(video, labels_dict, window_size=20): #video format: (#frames, 75, 4)\n",
    "    labels = video[\"class\"].iloc[0]\n",
    "    print(labels)\n",
    "    label = labels_dict[labels]\n",
    "\n",
    "    video.drop(columns=df.columns[0], axis=1, inplace = True)\n",
    "    \n",
    "    # dfList = video.values.tolist()\n",
    "    dfList = video.to_numpy().reshape((len(video), 75, 4)).tolist()\n",
    "    # type(dfList)\n",
    "\n",
    "    \n",
    "    finalLabels = [[label]]\n",
    "    data = []\n",
    "    temp = dfList[0: window_size]\n",
    "    # print(temp[19])\n",
    "    # print(type(temp))\n",
    "    data.append(temp)\n",
    "\n",
    "    for i in range(window_size, len(video)):\n",
    "        # print(type(temp))\n",
    "        \n",
    "        temp.pop(0)\n",
    "        # print(type(temp))\n",
    "        temp.append(dfList[i])\n",
    "        # print(type(temp))\n",
    "        # print(\"df list:\",type(dfList))\n",
    "        finalLabels.append([label])\n",
    "        data.append(temp)\n",
    "\n",
    "    return np.array(data), labels\n",
    "\n",
    "\n",
    "#past code, the code on top does the same\n",
    "    # def first_continuity(df, FrameNumber):\n",
    "    #     ls = df.iloc[i:i+FrameNumber].values.tolist()\n",
    "    #     return ls\n",
    "\n",
    "    # def modify_continuity(tmp_ls, next_row):\n",
    "    #     tmp_ls.append(next_row)\n",
    "    #     tmp_ls.pop(0)\n",
    "    #     return tmp_ls\n",
    "\n",
    "    # def time_continuity(df, FrameNumber):\n",
    "    #     finalList = []\n",
    "\n",
    "    #     labels, df = get_label(df, FrameNumber)\n",
    "        \n",
    "    #     tmp_list = first_continuity(df, FrameNumber)\n",
    "    #     finaList.append(tmp_list)\n",
    "\n",
    "    #     dataList = df.values.tolist()\n",
    "    #     print(\"The number of labels is \", len(labels))\n",
    "    #     size = len(dataList-FrameNumber)\n",
    "    #     print(\"The number of datapoints is \" , size)\n",
    "\n",
    "    #     for i in range(FrameNumber, len(dataList)-1):\n",
    "    #         tmp_list = modify_continuity(tmp_list, dataList[i])\n",
    "    #         finalList.append(tmp_list)\n",
    "\n",
    "    #     return finalList\n",
    "\n",
    "#---- SARINA, uncomment DOWN HERE------#\n",
    "    #-------------------------------------------\n",
    "    # label = df[0].iloc[0]\n",
    "    # print(\"HI\")\n",
    "    # previous_frames = [label]\n",
    "    # #saving the first n frames:\n",
    "    # headerList = header(FrameNumber, label)\n",
    "    # new_df = pd.DataFrame(columns = headerList)\n",
    "    \n",
    "    # for row in range(FrameNumber):\n",
    "    #     i=0\n",
    "    #     for el in row:\n",
    "    #         if i==0:\n",
    "    #             i+=1\n",
    "    #             continue\n",
    "    #         previous_frames.append(df[el].iloc[row])\n",
    "        \n",
    "    # for row in range(FrameNumber, len(df)):\n",
    "    #     new_df.concat(previous_frames)\n",
    "    #     #removing the first n elements.\n",
    "    #     previous_frames = previous_frames[len(headerList):]\n",
    "    #     previous_frames.push_front(label)\n",
    "\n",
    "    #     i=0\n",
    "    #     for el in row:\n",
    "    #         if i==0:\n",
    "    #             #skip the class name\n",
    "    #             i+=1\n",
    "    #             continue\n",
    "    #         previous_frames.append(df[el].iloc[row])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new create_data function\n",
    "\n",
    "def new_create_data(video, label_number, window_size=20):\n",
    "    final_labels = [label_number]*(len(video)-window_size)\n",
    "    \n",
    "    data = []\n",
    "    temp = video[0: window_size]\n",
    "    for i in range(window_size, len(video)):\n",
    "        temp.tolist().pop(0).append(video[i])\n",
    "        data.append(temp)\n",
    "        \n",
    "    return np.array(data), np.array(final_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]],\n",
       " \n",
       "        [[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]],\n",
       " \n",
       "        [[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]],\n",
       " \n",
       "        [[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]],\n",
       " \n",
       "        [[ 0.60614169,  0.24424243, -0.81565225, ...,  0.70523119,\n",
       "          -0.02412888,  0.        ],\n",
       "         [ 0.60600853,  0.24425413, -0.81574887, ...,  0.70002681,\n",
       "          -0.02327156,  0.        ],\n",
       "         [ 0.60510093,  0.24443686, -0.75444818, ...,  0.69271201,\n",
       "          -0.02284604,  0.        ],\n",
       "         ...,\n",
       "         [ 0.59127599,  0.24489416, -0.75127196, ...,  0.69656569,\n",
       "          -0.02241407,  0.        ],\n",
       "         [ 0.59114814,  0.24478841, -0.752092  , ...,  0.69716346,\n",
       "          -0.02378356,  0.        ],\n",
       "         [ 0.5910871 ,  0.24469003, -0.75246221, ...,  0.6966213 ,\n",
       "          -0.02494952,  0.        ]]]),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]),\n",
       " (48, 20, 300),\n",
       " (68, 300),\n",
       " (48, 1))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sarina's test\n",
    "print(type(X_train[0]))\n",
    "new_data_testing, label_test = new_create_data(X_train[0], Y_train[0])\n",
    "new_data_testing, label_test, new_data_testing.shape, X_train[0].shape, label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# df = pd.read_csv(\"test-file.csv\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m X_train[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m dfList \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      4\u001b[0m npData, labels \u001b[39m=\u001b[39m create_data(video \u001b[39m=\u001b[39m df, labels_dict \u001b[39m=\u001b[39m dicti, window_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m )\n\u001b[0;32m      5\u001b[0m \u001b[39m# all_frames = df.to_numpy().reshape((len(df), 75, 4))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m# finalList = time_continuity(df, 10)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# # df.head()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# # df.head(100)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"test-file.csv\")\n",
    "df = X_train[0]\n",
    "dfList = df.values.tolist()\n",
    "npData, labels = create_data(video = df, labels_dict = dicti, window_size=20 )\n",
    "# all_frames = df.to_numpy().reshape((len(df), 75, 4))\n",
    "\n",
    "# finalList = time_continuity(df, 10)\n",
    "# # print(finalList)\n",
    "# # type(finalList)\n",
    "# print(len(finalList), len(finalList[0]), len(finalList[0][0]))\n",
    "# # df.head()\n",
    "# # df.head(100)\n",
    "npData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameNumber = 10\n",
    "time_continuity(train_df, FrameNumber = 2)\n",
    "time_continuity(test_df, FrameNumber = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(X_train)):\n\u001b[0;32m      7\u001b[0m     data, labels \u001b[39m=\u001b[39m new_create_data(X_train[i], Y_train[i])\n\u001b[1;32m----> 8\u001b[0m     train_df \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((train_df, data))\n\u001b[0;32m      9\u001b[0m     train_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((train_labels, labels))\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(X_test)):\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df, train_labels = new_create_data(X_train[0], Y_train[0])\n",
    "\n",
    "test_df, test_labels = new_create_data(X_test[0], Y_test[0])\n",
    "#we can implement better ways to predict on the test_df, this is to check overall accuracy\n",
    "\n",
    "for i in range(1, len(X_train)):\n",
    "    data, labels = new_create_data(X_train[i], Y_train[i])\n",
    "    train_df = np.concatenate((train_df, data))\n",
    "    train_labels = np.concatenate((train_labels, labels))\n",
    "    \n",
    "for i in range(1, len(X_test)):\n",
    "    data, labels = new_create_data(X_test[i], Y_test[i])\n",
    "    test_df = np.concatenate((test_df, data) )\n",
    "    test_labels = np.concatenate((test_labels, labels))\n",
    "    \n",
    "train_df = np.array(train_df)\n",
    "train_labels = np.array(train_labels)\n",
    "test_df = np.array(test_df)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_df.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### separating the features from the labels (just some testing, not actually used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_label(df):\n",
    "    df_features = df.copy()\n",
    "\n",
    "    df_labels = df_features.pop(\"class\")\n",
    "\n",
    "    df_features = np.array(df_features)\n",
    "    # df_labels = np.array(df_labels)\n",
    "    print(df_features.shape)\n",
    "    print(df_labels.shape, type(df_labels))\n",
    "    return df_features, df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = seperate_label(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = seperate_label(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n",
    "#           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n",
    "#           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n",
    "# df = pd.DataFrame(mydict)\n",
    "# print(df.head())\n",
    "# mylist = []\n",
    "# mylist.append(df.iloc[1])\n",
    "# for i in mylist:\n",
    "#     print(i)\n",
    "#     # for l in i:\n",
    "#     #     print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I\"ll also remap every string to an int\n",
    "per example, \n",
    "through a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_label(df_labels):    \n",
    "#should be between 0 and 3\n",
    "    dicti={\"coffee\": 0, \"dog\": 1, \"door\": 2, \"milk\": 3}\n",
    "    # for element in df_labels:\n",
    "    #     element = dicti[element]\n",
    "\n",
    "    def change_label(x):\n",
    "        return dicti[x]\n",
    "\n",
    "    df_labels = df_labels.apply(change_label)\n",
    "    df_labels.head()\n",
    "    return df_labels\n",
    "\n",
    "Y_train = reformat_label(Y_train)\n",
    "Y_val = reformat_label(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have to make each label a single vector \n",
    "# I can just rotate (transpose, since now I have a single rolumn, I will make it into many rows)! \n",
    "Y_train = np.array([Y_train]).T\n",
    "Y_val = np.array([Y_val]).T\n",
    "print(Y_train.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_split\n",
    "\n",
    "##### I'll split into train and val, and then tensorflow will split for me in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_val,Y_val = shuffle(X_val, Y_val, random_state=0)\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(20, 300)),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dropout(0.1), \n",
    "    # Dense(units=128, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    # Dropout(0.1), \n",
    "    Dense(units=64, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dense(units=64, activation='relu',\n",
    "          kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 6000)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               768128    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 780,804\n",
      "Trainable params: 780,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoints are like saving the model, but at different epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model_checkpoints/augmented.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "239/239 [==============================] - 3s 9ms/step - loss: 13.1801 - accuracy: 0.4240 - val_loss: 2.6428 - val_accuracy: 0.4452\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.2890 - accuracy: 0.4603 - val_loss: 2.4610 - val_accuracy: 0.3220\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.1595 - accuracy: 0.4739 - val_loss: 2.4539 - val_accuracy: 0.3824\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.1435 - accuracy: 0.4743 - val_loss: 2.3737 - val_accuracy: 0.3251\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0996 - accuracy: 0.4793 - val_loss: 2.3825 - val_accuracy: 0.3957\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0810 - accuracy: 0.4899 - val_loss: 2.3695 - val_accuracy: 0.3441\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0573 - accuracy: 0.5028 - val_loss: 2.4095 - val_accuracy: 0.4201\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0695 - accuracy: 0.4988 - val_loss: 2.4212 - val_accuracy: 0.3692\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 2.0588 - accuracy: 0.4935 - val_loss: 2.4084 - val_accuracy: 0.3848\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0543 - accuracy: 0.5047 - val_loss: 2.4195 - val_accuracy: 0.4404\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0592 - accuracy: 0.5084 - val_loss: 2.4014 - val_accuracy: 0.3848\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0668 - accuracy: 0.4975 - val_loss: 2.4065 - val_accuracy: 0.3848\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0561 - accuracy: 0.5098 - val_loss: 2.4143 - val_accuracy: 0.3848\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 2.0403 - accuracy: 0.5162 - val_loss: 2.4252 - val_accuracy: 0.3777\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 2.0540 - accuracy: 0.5088 - val_loss: 2.3593 - val_accuracy: 0.4642\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0449 - accuracy: 0.5206 - val_loss: 2.4749 - val_accuracy: 0.3410\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0386 - accuracy: 0.5195 - val_loss: 2.3993 - val_accuracy: 0.4398\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0400 - accuracy: 0.5199 - val_loss: 2.4263 - val_accuracy: 0.4201\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 2.0397 - accuracy: 0.5178 - val_loss: 2.4713 - val_accuracy: 0.4021\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 2.0325 - accuracy: 0.5275 - val_loss: 2.5292 - val_accuracy: 0.3118\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(train_df,train_labels, epochs=20, validation_data=(test_df,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val,Y_val)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = np.array([[0.4981820583343506,0.3041764795780182,-0.39641207456588745,0.9999492764472961,0.5154309272766113,0.2550220787525177,-0.34935909509658813,0.9998473525047302,0.5238351225852966,0.2552952766418457,-0.34945884346961975,0.9998874068260193,0.5324825644493103,0.25629663467407227,-0.3490661680698395,0.9998465776443481,0.480642706155777,0.258960485458374,-0.35233527421951294,0.9998729228973389,0.46913933753967285,0.26167136430740356,-0.3522539734840393,0.9999103546142578,0.4588560163974762,0.26559099555015564,-0.35270392894744873,0.9998877644538879,0.5477309823036194,0.2884930372238159,-0.08250369131565094,0.9998624324798584,0.44203734397888184,0.2937893569469452,-0.08980625122785568,0.9999209046363831,0.5207302570343018,0.3572015166282654,-0.2996494770050049,0.9999455809593201,0.4778904318809509,0.35783419013023376,-0.303683340549469,0.9999536871910095,0.6314497590065002,0.5698841214179993,0.008182904683053493,0.9999152421951294,0.37602466344833374,0.5611208081245422,0.020433634519577026,0.9998253583908081,0.7292965650558472,0.9798671007156372,-0.32061514258384705,0.9971438646316528,0.20657332241535187,0.8604757785797119,-0.35482174158096313,0.9956580996513367,0.5498597621917725,0.8489202857017517,-0.7637590169906616,0.9986798167228699,0.37362605333328247,0.6491265296936035,-0.8803924322128296,0.9952695369720459,0.4984210431575775,0.8526077270507812,-0.8525819778442383,0.9939749836921692,0.4279037117958069,0.6301683187484741,-0.9744688868522644,0.9816589951515198,0.49681568145751953,0.7763991355895996,-0.7786155939102173,0.9938353300094604,0.43934187293052673,0.5735072493553162,-0.9105846285820007,0.9798092246055603,0.5083235502243042,0.7676472067832947,-0.7398286461830139,0.991555392742157,0.4291306436061859,0.581972599029541,-0.8635354042053223,0.9797517657279968,0.5966092348098755,1.2567808628082275,-0.008524066768586636,0.1589588075876236,0.38996055722236633,1.2599668502807617,0.010765359736979008,0.17815829813480377,0.5948789715766907,1.8001701831817627,0.003953699953854084,0.012086698785424232,0.4031899869441986,1.7931101322174072,-0.015372313559055328,0.00784522294998169,0.6021834015846252,2.2724609375,0.30393192172050476,0.0014408509014174342,0.42258867621421814,2.2609975337982178,0.15069334208965302,0.0005837850621901453,0.6074386835098267,2.337127685546875,0.3106561303138733,0.0007881768397055566,0.41845861077308655,2.323322296142578,0.15600086748600006,0.0007185556460171938,0.5843966603279114,2.4317216873168945,-0.0522509329020977,0.0006601922796107829,0.4535377621650696,2.422938585281372,-0.2161559760570526,0.001258615986444056,0.373634397983551,0.6499707102775574,-1.8444630711655918e-07,0.0,0.39679813385009766,0.5777179598808289,0.0013798748841509223,0.0,0.43735894560813904,0.534506618976593,-0.00420121755450964,0.0,0.47639840841293335,0.5254983901977539,-0.01024630106985569,0.0,0.5027356743812561,0.5411986708641052,-0.014173091389238834,0.0,0.45060163736343384,0.5403348207473755,-0.015738490968942642,0.0,0.5017736554145813,0.5638222098350525,-0.02503901533782482,0.0,0.4914117455482483,0.5653451681137085,-0.027088096365332603,0.0,0.47413501143455505,0.564212441444397,-0.0264134518802166,0.0,0.4494023323059082,0.5912742614746094,-0.021495746448636055,0.0,0.5018069744110107,0.6102883815765381,-0.025984806939959526,0.0,0.4879333972930908,0.6084942817687988,-0.0215410478413105,0.0,0.46763357520103455,0.6072394847869873,-0.018301531672477722,0.0,0.44739097356796265,0.6418318152427673,-0.02643604204058647,0.0,0.4959167242050171,0.6526373028755188,-0.02697780914604664,0.0,0.48271769285202026,0.6494451761245728,-0.015943871811032295,0.0,0.46350032091140747,0.6479849815368652,-0.009380415081977844,0.0,0.4440409541130066,0.686765730381012,-0.03130561113357544,0.0,0.48238319158554077,0.6835903525352478,-0.027893563732504845,0.0,0.4705689549446106,0.6809285879135132,-0.01684371940791607,0.0,0.4541867673397064,0.6818298101425171,-0.009424922056496143,0.0,0.5541528463363647,0.8231633305549622,-1.2188849041194771e-07,0.0,0.5286245942115784,0.7567625045776367,0.002618174534291029,0.0,0.490420401096344,0.725986659526825,-0.0019848633091896772,0.0,0.45551788806915283,0.727816104888916,-0.006525832694023848,0.0,0.4336313307285309,0.7500261068344116,-0.011036446318030357,0.0,0.4799780249595642,0.7299957275390625,-0.022469153627753258,0.0,0.4273468255996704,0.7438825368881226,-0.032165635377168655,0.0,0.4391612410545349,0.7527205348014832,-0.0347185879945755,0.0,0.4578348398208618,0.7505552768707275,-0.034520745277404785,0.0,0.4814111292362213,0.783324658870697,-0.027409298345446587,0.0,0.4299897849559784,0.79166579246521,-0.032714203000068665,0.0,0.4421943128108978,0.7919579744338989,-0.02897382713854313,0.0,0.4601382911205292,0.789323091506958,-0.02673531323671341,0.0,0.4840109646320343,0.8338181972503662,-0.03076060861349106,0.0,0.4401523470878601,0.8343431949615479,-0.032595742493867874,0.0,0.45229482650756836,0.8290730118751526,-0.02321871742606163,0.0,0.467772513628006,0.826426088809967,-0.01786368153989315,0.0,0.4874705970287323,0.8772450685501099,-0.03356040641665459,0.0,0.4529787600040436,0.8700811266899109,-0.03298313543200493,0.0,0.4627114236354828,0.8613516688346863,-0.02354632131755352,0.0,0.4751517176628113,0.8603678345680237,-0.016946855932474136,0.0]])\n",
    "#this is coffee\n",
    "\n",
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('accuracy_4_labels_regularization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('loss_4_labels_regularization.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THis is the model I had written in the past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=4, activation='softmax')\n",
    "])\n",
    "model2.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(X_train,Y_train, epochs=20, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('accuracy_4_labels_no_regularization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('loss_4_labels_no_regularization.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "84b29276fa8760bc050710a9f985725ad5c92bfe96b2ef6a88d60bdfd3b907cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
