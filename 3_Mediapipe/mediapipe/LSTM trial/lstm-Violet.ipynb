{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(file):\n",
    "    try:\n",
    "     csv_data = pd.read_csv(file)\n",
    "    except:\n",
    "         print(\"CANT READ CSV: \", file)\n",
    "         return\n",
    "    \n",
    "    if csv_data.isnull().values.any():\n",
    "            \n",
    "            return False\n",
    "    try:\n",
    "        csv_data = csv_data.drop(\"class\", axis = 1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return (csv_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: /Tmp/ASL-data/Database-augmented/coffee/: No such file or directory\n",
      " capture.txt   example.txt\t lstm_25_labels.ipynb   lstm-testing-model.py\n",
      " demo_test    'lstm (1).ipynb'\t lstm.ipynb\n",
      " capture.txt   example.txt\t lstm_25_labels.ipynb   lstm-testing-model.py\n",
      " demo_test    'lstm (1).ipynb'\t lstm.ipynb\n"
     ]
    }
   ],
   "source": [
    "!cd /Tmp/ASL-data/Database-augmented/coffee/; ls;\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: /Tmp/linxinle/ASL-data/Database-augmented/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cd /Tmp/linxinle/ASL-data/Database-augmented/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48266143,  0.25151151, -0.66583532, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.48195514,  0.25146753, -0.66424268, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.48045793,  0.25147596, -0.65664881, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.48019916,  0.24556774, -0.64471287, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.48011598,  0.24555989, -0.65619022, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.48010522,  0.24555062, -0.66737396, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file = \"/Tmp/linxinle/ASL-data/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
    "# file = \"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
    "file = \"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train/book/41X2t_s2Ai40.csv\"\n",
    "process_csv_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_padding(arr, max_length):\n",
    "    arr = np.append(arr, np.zeros((max_length-arr.shape[0],300)), axis=0)\n",
    "    return np.expand_dims(arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(folder_path, labels, train=True):\n",
    "    total = 0\n",
    "    max_length = 211\n",
    "    \n",
    "    # find max length\n",
    "    # for folder in os.listdir(folder_path):\n",
    "    #     print(folder)\n",
    "    #     if folder != \".DS_Store\":\n",
    "    #         total += len(os.listdir(os.path.join(folder_path, folder)))\n",
    "    #         for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "    #             if file != \".DS_Store\":\n",
    "    #                 data = process_csv_file(os.path.join(folder_path, folder, file))\n",
    "    #                 if data is not(False):\n",
    "    #                     if data.shape[0] > max_length:\n",
    "    #                         max_length = data.shape[0]\n",
    "    # print(max_length)\n",
    "    # Make the arrays\n",
    "    X = np.empty((0, max_length, 300))\n",
    "    Y = np.empty((0,), int)\n",
    "    print(\"----------\")\n",
    "    for folder in os.listdir(folder_path):\n",
    "        print(folder)\n",
    "        if folder != \".DS_Store\":\n",
    "            i=0\n",
    "            for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                i+=1\n",
    "                if file != \".DS_Store\": \n",
    "                    data = process_csv_file(os.path.join(folder_path, folder, file))\n",
    "                    if data is not(False):\n",
    "                        data = set_padding(data, max_length)\n",
    "                        X = np.append(X, data, axis=0)\n",
    "                        Y = np.append(Y, labels[folder])\n",
    "                        if (i% 50 == 0):\n",
    "                            print(i)\n",
    "                if i == 5:\n",
    "                    break\n",
    "                    \n",
    "    print(total, max_length)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hear': 0,\n",
       " 'music': 1,\n",
       " 'bookstore': 2,\n",
       " 'classroom': 3,\n",
       " 'doctor': 4,\n",
       " 'focus': 5,\n",
       " 'chicken': 6,\n",
       " 'door': 7,\n",
       " 'many': 8,\n",
       " 'polite': 9,\n",
       " 'good morning': 10,\n",
       " 'coffee': 11,\n",
       " 'photographer': 12,\n",
       " 'hamburger': 13,\n",
       " 'i': 14,\n",
       " 'phone': 15,\n",
       " 'brother': 16,\n",
       " 'i love you': 17,\n",
       " 'milk': 18,\n",
       " 'dog': 19,\n",
       " 'ocean': 20,\n",
       " 'research': 21,\n",
       " 'book': 22,\n",
       " 'open': 23,\n",
       " 'money': 24}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "def get_labels(folder_path):\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    for folder in os.listdir(folder_path):\n",
    "            labels[folder] = count\n",
    "            count += 1\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(\"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "hear\n",
      "music\n",
      "bookstore\n",
      "classroom\n",
      "doctor\n",
      "focus\n",
      "chicken\n",
      "door\n",
      "many\n",
      "polite\n",
      "good morning\n",
      "coffee\n",
      "photographer\n",
      "hamburger\n",
      "i\n",
      "phone\n",
      "brother\n",
      "i love you\n",
      "milk\n",
      "dog\n",
      "ocean\n",
      "research\n",
      "book\n",
      "open\n",
      "money\n",
      "0 211\n"
     ]
    }
   ],
   "source": [
    "folder_path_train = \"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train\"\n",
    "# folder_path_train=\"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/data_25_labels_augmentation/train/\"\n",
    "X_train, Y_train = prepare_data(folder_path_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "hear\n",
      "music\n",
      "bookstore\n",
      "classroom\n",
      "doctor\n",
      "focus\n",
      "chicken\n",
      "door\n",
      "many\n",
      "polite\n",
      "good morning\n",
      "coffee\n",
      "photographer\n",
      "hamburger\n",
      "i\n",
      "phone\n",
      "brother\n",
      "i love you\n",
      "milk\n",
      "dog\n",
      "ocean\n",
      "research\n",
      "book\n",
      "open\n",
      "money\n",
      "0 211\n"
     ]
    }
   ],
   "source": [
    "# folder_path_test = \"/Tmp/ASL-data/Database-augmented/test\"\n",
    "folder_path_test = \"/Tmp/linxinle/ASL-data/Database-augmented/test\"\n",
    "folder_path_test = \"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train\"\n",
    "# labels = {\"coffee\": 0, 'dog': 1, 'milk': 2, 'door': 3}\n",
    "X_test, Y_test = prepare_data(folder_path_test, labels, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "], padding=\"post\"\n",
    ")\n",
    "# print(X[0][1])\n",
    "print(((padded_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Y = np.expand_dims(Y, axis=0)\n",
    "# Y= Y.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.57075262,  0.23228616, -0.7571311 , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.57188809,  0.2330122 , -0.78263861, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.57207823,  0.23495021, -0.80215025, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.52638198,  0.26888801, -0.82134467, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.52746353,  0.2689094 , -0.84388655, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.52761596,  0.26939814, -0.86171532, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.71679253,  0.19825935, -0.51778775, ...,  0.00383903,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.71713571,  0.19806704, -0.52774298, ...,  0.00383903,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.71760997,  0.19803804, -0.55183041, ...,  0.00383903,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.93761323,  0.42639963, -0.34276149, ...,  0.13842953,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.93713981,  0.42777433, -0.37061816, ...,  0.13842953,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.9369296 ,  0.42872254, -0.3315548 , ...,  0.13842953,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.68052722,  0.35269559, -0.86599219, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.68155759,  0.35139013, -1.03714895, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.68494805,  0.34935442, -1.05101061, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.5441556 ,  0.36708792, -0.81590664, ...,  1.0335939 ,\n",
       "          -0.02064314,  0.        ],\n",
       "         [ 0.54454434,  0.36663725, -0.78299844, ...,  1.05125178,\n",
       "          -0.02658421,  0.        ],\n",
       "         [ 0.54497242,  0.36663221, -0.7887159 , ...,  1.06007161,\n",
       "          -0.02668461,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]]),\n",
       " array([18, 18, 20,  1, 18,  4,  1,  2,  9, 14,  6, 23,  0, 10, 24, 15,  6,\n",
       "         9, 17, 15, 13, 21,  3,  4, 23,  2, 13, 15,  5, 24, 10, 15, 11, 12,\n",
       "         0, 20,  1, 11, 20, 19, 23, 10, 21, 16,  5,  3,  2, 12, 22, 20, 16,\n",
       "        12,  0, 19,  8,  8,  0,  3,  3, 11,  8,  7,  1, 11, 23, 14,  0,  6,\n",
       "         5, 11,  7,  4,  6, 19, 12, 19, 21,  6, 21,  2, 20,  3,  5, 10, 17,\n",
       "        24, 24, 16, 14, 17,  4, 23, 15, 16,  5,  7, 17, 22,  9, 22,  7, 13,\n",
       "        12,  2, 22, 18, 14, 18,  7,  4, 17,  1, 21, 24, 14, 13, 10,  9]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_val,Y_val = shuffle(X_test, Y_test, random_state=0)\n",
    "X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118, 211, 300), (118,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:21:38.239567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.239830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.240005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.647647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.647803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.647917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 23:21:38.648018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 332 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
      "2023-07-07 23:21:38.816872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:38.817575: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:38.818194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:38.934424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:38.935344: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:38.935999: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "#LSTM model - 4 labels\n",
    "from tensorflow.keras import layers\n",
    "x_shape = (457, 211, 300)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(211, 300)))\n",
    "# model_lstm.add(Dropout(0.2))\n",
    "# model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(64))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:21:43.870011: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:43.870733: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:43.871487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:43.985462: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:43.986160: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:43.986771: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "#LSTM model - 25 labels\n",
    "from tensorflow.keras import layers\n",
    "x_shape = (457, 211, 300)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(211, 300)))\n",
    "# model_lstm.add(Dropout(0.2))\n",
    "# model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(64, return_sequences=True))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(0.05)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(128, activation=\"relu\"))\n",
    "model_lstm.add(Dense(64, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(0.05)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(25, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"/Tmp/shariffa/LSTM-25-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "#     \"/Tmp/linxinle/models/LSTM-25-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:21:47.203176: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:47.204143: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:47.204810: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:47.316380: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:47.317138: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:47.317821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:48.118785: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:48.119812: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:48.120500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:48.231452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-07 23:21:48.232234: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-07 23:21:48.232908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-07 23:21:49.318046: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-07-07 23:21:49.318124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 525.116.4\n",
      "2023-07-07 23:21:49.318145: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n",
      "2023-07-07 23:21:49.318168: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Fail to find the dnn implementation.\n",
      "\t [[{{node CudnnRNN}}]]\n",
      "2023-07-07 23:21:49.318234: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: {{function_node __forward_gpu_lstm_with_fallback_5873_specialized_for_sequential_1_lstm_2_PartitionedCall_at___inference_train_function_7841}} {{function_node __forward_gpu_lstm_with_fallback_5873_specialized_for_sequential_1_lstm_2_PartitionedCall_at___inference_train_function_7841}} Fail to find the dnn implementation.\n",
      "\t [[{{node CudnnRNN}}]]\n",
      "\t [[sequential_1/lstm_2/PartitionedCall]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm_2/PartitionedCall]] [Op:__inference_train_function_7841]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_lstm\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_lstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Tmp/shariffa/miniconda3/envs/asl-converter/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Tmp/shariffa/miniconda3/envs/asl-converter/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm_2/PartitionedCall]] [Op:__inference_train_function_7841]"
     ]
    }
   ],
   "source": [
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model_lstm.fit(X_train, Y_train, epochs=50, validation_data=(X_val, Y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl-converter",
   "language": "python",
   "name": "asl-converter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
