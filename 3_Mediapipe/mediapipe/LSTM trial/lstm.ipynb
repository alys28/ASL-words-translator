{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_file(file):\n",
        "    try:\n",
        "     csv_data = pd.read_csv(file)\n",
        "    except:\n",
        "         print(\"CANT READ CSV: \", file)\n",
        "         return\n",
        "    \n",
        "    if csv_data.isnull().values.any():\n",
        "            \n",
        "            return False\n",
        "    try:\n",
        "        csv_data = csv_data.drop(\"class\", axis = 1)\n",
        "    except KeyError:\n",
        "        pass\n",
        "    return (csv_data.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: cd: /Tmp/ASL-data/Database-augmented/coffee/: No such file or directory\n",
            "2_Coding-Competition  ASL-words-translator  ASL-words-translator_old\n",
            "2_Coding-Competition  ASL-words-translator  ASL-words-translator_old\n"
          ]
        }
      ],
      "source": [
        "!cd /Tmp/ASL-data/Database-augmented/coffee/; ls;\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cd /Tmp/linxinle/ASL-data/Database-augmented/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CANT READ CSV:  /media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/Database-augmented/test/coffee/2sGQuduhAf41354.csv\n"
          ]
        }
      ],
      "source": [
        "# file = \"/Tmp/linxinle/ASL-data/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
        "file = \"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
        "process_csv_file(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_padding(arr, max_length):\n",
        "    arr = np.append(arr, np.zeros((max_length-arr.shape[0],300)), axis=0)\n",
        "    return np.expand_dims(arr, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(folder_path, labels, train=True):\n",
        "    total = 0\n",
        "    max_length = 211\n",
        "    \n",
        "    # find max length\n",
        "    # for folder in os.listdir(folder_path):\n",
        "    #     print(folder)\n",
        "    #     if folder != \".DS_Store\":\n",
        "    #         total += len(os.listdir(os.path.join(folder_path, folder)))\n",
        "    #         for file in os.listdir(os.path.join(folder_path, folder)):\n",
        "    #             if file != \".DS_Store\":\n",
        "    #                 data = process_csv_file(os.path.join(folder_path, folder, file))\n",
        "    #                 if data is not(False):\n",
        "    #                     if data.shape[0] > max_length:\n",
        "    #                         max_length = data.shape[0]\n",
        "    # print(max_length)\n",
        "    # Make the arrays\n",
        "    X = np.empty((0, max_length, 300))\n",
        "    Y = np.empty((0,), int)\n",
        "    print(\"----------\")\n",
        "    for folder in os.listdir(folder_path):\n",
        "        print(folder)\n",
        "        if folder != \".DS_Store\":\n",
        "            \n",
        "            i=0\n",
        "            for file in os.listdir(os.path.join(folder_path, folder)):\n",
        "                i+=1\n",
        "                if file != \".DS_Store\": \n",
        "                    data = process_csv_file(os.path.join(folder_path, folder, file))\n",
        "                    if data is not(False):\n",
        "                        data = set_padding(data, max_length)\n",
        "                        X = np.append(X, data, axis=0)\n",
        "                        Y = np.append(Y, labels[folder])\n",
        "                        if (i% 50 == 0):\n",
        "                            print(i)\n",
        "                if i==5:\n",
        "                    break\n",
        "\n",
        "          \n",
        "    print(total, max_length)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "dog\n",
            "door\n",
            "coffee\n",
            "milk\n",
            "0 211\n"
          ]
        }
      ],
      "source": [
        "folder_path_train = \"/Tmp/linxinle/ASL-data/Database-augmented/train\"\n",
        "# folder_path_train=\"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/data_25_labels_augmentation/train/\"\n",
        "labels= {\"coffee\": 0, 'dog': 1, 'milk': 2, 'door': 3}\n",
        "X_train, Y_train = prepare_data(folder_path_train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "dog\n",
            "door\n",
            "coffee\n",
            "milk\n",
            "0 211\n"
          ]
        }
      ],
      "source": [
        "# folder_path_test = \"/Tmp/ASL-data/Database-augmented/test\"\n",
        "folder_path_test = \"/Tmp/linxinle/ASL-data/Database-augmented/test\"\n",
        "\n",
        "labels = {\"coffee\": 0, 'dog': 1, 'milk': 2, 'door': 3}\n",
        "X_test, Y_test = prepare_data(folder_path_test, labels, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 711  632   71    0    0    0]\n",
            " [  73    8 3215   55  927    0]\n",
            " [  83   91    1  645 1253  927]]\n"
          ]
        }
      ],
      "source": [
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    [\n",
        "    [711, 632, 71],\n",
        "    [73, 8, 3215, 55, 927],\n",
        "    [83, 91, 1, 645, 1253, 927],\n",
        "], padding=\"post\"\n",
        ")\n",
        "# print(X[0][1])\n",
        "print(((padded_inputs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Y = np.expand_dims(Y, axis=0)\n",
        "# Y= Y.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[[ 0.13714732,  0.21640315, -0.76089728, ...,  0.03080839,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.13900962,  0.21570578, -0.70383424, ...,  0.02660934,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.14016245,  0.21600241, -0.68726504, ...,  0.0246705 ,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              " \n",
              "        [[ 0.03935957, -0.55007347, -0.82141459, ..., -0.9058095 ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.03496937, -0.55671575, -0.88811451, ..., -0.91157166,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.22295145, -0.25609224, -0.88870674, ..., -0.60995682,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              " \n",
              "        [[-0.3046777 , -0.28868665, -0.77814454, ..., -0.59919764,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.30105557, -0.28883495, -0.79228008, ..., -0.59946623,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.2995231 , -0.28909438, -0.80081856, ..., -0.59997483,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-0.2093778 , -0.28535029, -0.63905609, ..., -0.54705716,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.20621122, -0.2939713 , -0.75043589, ..., -0.55557748,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.21052193, -0.28923716, -0.79280788, ..., -0.5508704 ,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              " \n",
              "        [[-0.30128099, -0.6530865 , -0.72565377, ..., -0.8651643 ,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.30220679, -0.65385142, -0.65911466, ..., -0.8684044 ,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.30305266, -0.64979367, -0.52304482, ..., -0.86620575,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              " \n",
              "        [[-0.25597251, -0.1769882 , -0.63651919, ..., -0.32282972,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.25677705, -0.17738821, -0.63760769, ..., -0.32305099,\n",
              "           0.        ,  0.        ],\n",
              "         [-0.25711382, -0.17787226, -0.63930351, ..., -0.32356053,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]]]),\n",
              " array([2, 1, 2, 3, 0, 2, 3, 0, 1, 1, 3, 0, 3, 3, 2, 0, 1, 1, 2, 0]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shuffle data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
        "X_val,Y_val = shuffle(X_test, Y_test, random_state=0)\n",
        "X_train, Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((20, 211, 300), (20,))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_shape = X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 211)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "masking_input = []\n",
        "for i in range(x_shape[0]):\n",
        "    masking_input.append(X_train[i].T[0])\n",
        "\n",
        "masking_input = np.array(masking_input)\n",
        "\n",
        "masking_input.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " ...\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]], shape=(20, 211), dtype=bool)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-19 18:43:38.595080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.595356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.595543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.999251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.999411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.999529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-19 18:43:38.999632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 332 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20, 211, 211), dtype=float32, numpy=\n",
              "array([[[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]],\n",
              "\n",
              "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]],\n",
              "\n",
              "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]],\n",
              "\n",
              "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]],\n",
              "\n",
              "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        ...,\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343],\n",
              "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
              "         -0.0169578 , -0.02144343]]], dtype=float32)>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = layers.Embedding(input_dim=x_shape[1], output_dim=x_shape[1], mask_zero=True)\n",
        "masked_output = embedding(masking_input)\n",
        "\n",
        "print(masked_output._keras_mask)\n",
        "masked_output\n",
        "# masking_layer = layers.Masking()\n",
        "# # Simulate the embedding lookup by expanding the 2D input to 3D,\n",
        "# # with embedding dimension of 10.\n",
        "# unmasked_embedding = tf.cast(\n",
        "#     tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
        "# )\n",
        "\n",
        "# masked_embedding = masking_layer(unmasked_embedding)\n",
        "# print(masked_embedding._keras_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-19 18:45:01.867637: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-06-19 18:45:01.868345: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-06-19 18:45:01.869086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        }
      ],
      "source": [
        "#LSTM model \n",
        "from tensorflow.keras import layers\n",
        "x_shape = (457, 211, 300)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(211, 300)))\n",
        "# model_lstm.add(Dropout(0.2))\n",
        "# model_lstm.add(LSTM(128, return_sequences=True))\n",
        "# model_lstm.add(Dropout(0.2))\n",
        "# model_lstm.add(LSTM(64))\n",
        "model_lstm.add(Flatten())\n",
        "# model_lstm.add(Dense(128, activation=\"relu\"))\n",
        "# model_lstm.add(Dense(64, activation=\"relu\"))\n",
        "model_lstm.add(Dense(4, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    # \"/Tmp/shariffa/LSTM-4-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
        "    \"/Tmp/linxinle/models/LSTM-4-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
        "\n",
        "    monitor='val_accuracy',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    save_freq='epoch',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-19 18:45:06.458525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-06-19 18:45:06.459489: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-06-19 18:45:06.460176: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2023-06-19 18:45:06.804952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-06-19 18:45:06.805941: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-06-19 18:45:06.806638: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2023-06-19 18:45:07.257890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
            "2023-06-19 18:45:07.257911: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n",
            "2023-06-19 18:45:07.257918: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Fail to find the dnn implementation.\n",
            "\t [[{{node CudnnRNN}}]]\n",
            "2023-06-19 18:45:07.257942: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: {{function_node __forward_gpu_lstm_with_fallback_11351_specialized_for_sequential_1_lstm_3_PartitionedCall_at___inference_train_function_12073}} {{function_node __forward_gpu_lstm_with_fallback_11351_specialized_for_sequential_1_lstm_3_PartitionedCall_at___inference_train_function_12073}} Fail to find the dnn implementation.\n",
            "\t [[{{node CudnnRNN}}]]\n",
            "\t [[sequential_1/lstm_3/PartitionedCall]]\n"
          ]
        },
        {
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm_3/PartitionedCall]] [Op:__inference_train_function_12073]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_lstm\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model_lstm\u001b[39m.\u001b[39;49mfit(X_train, Y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, Y_val), callbacks\u001b[39m=\u001b[39;49m[checkpoint])\n",
            "File \u001b[0;32m/Tmp/linxinle/Programs/Miniconda3/envs/ASL_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/Tmp/linxinle/Programs/Miniconda3/envs/ASL_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm_3/PartitionedCall]] [Op:__inference_train_function_12073]"
          ]
        }
      ],
      "source": [
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model_lstm.fit(X_train, Y_train, epochs=50, validation_data=(X_val, Y_val), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=(x_shape[1], x_shape[2]))\n",
        "x = layers.GRU(64)(inputs, mask=masked_output._keras_mask)\n",
        "# x = layers.LSTM(64)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model_lstm2= tf.keras.Model(inputs, outputs)\n",
        "model_lstm2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm2.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ASL_env",
      "language": "python",
      "name": "asl_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
