{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 00:21:21.538521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 00:21:22.186057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 16:50:09.780886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-05 16:50:09.810296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-05 16:50:09.810590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(file):\n",
    "    try:\n",
    "     csv_data = pd.read_csv(file)\n",
    "    except:\n",
    "         print(\"CANT READ CSV: \", file)\n",
    "         return\n",
    "    \n",
    "    if csv_data.isnull().values.any():\n",
    "            \n",
    "            return False\n",
    "    try:\n",
    "        csv_data = csv_data.drop(\"class\", axis = 1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return (csv_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: /Tmp/ASL-data/Database-augmented/coffee/: No such file or directory\n",
      " capture.txt   example.txt\t lstm.ipynb\n",
      " demo_test    'lstm (1).ipynb'\t lstm-testing-model.py\n",
      " capture.txt   example.txt\t lstm.ipynb\n",
      " demo_test    'lstm (1).ipynb'\t lstm-testing-model.py\n"
     ]
    }
   ],
   "source": [
    "!cd /Tmp/ASL-data/Database-augmented/coffee/; ls;\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /Tmp/linxinle/ASL-data/Database-augmented/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78168657,  0.22384071, -0.7391246 , ...,  0.7758554 ,\n",
       "        -0.00941299,  0.        ],\n",
       "       [ 0.78184824,  0.22194237, -0.72576815, ...,  0.79439814,\n",
       "        -0.00434004,  0.        ],\n",
       "       [ 0.78186073,  0.22151854, -0.73018891, ...,  0.77655236,\n",
       "        -0.00194774,  0.        ],\n",
       "       ...,\n",
       "       [ 0.77380546,  0.22890084, -0.66333413, ...,  0.57258446,\n",
       "        -0.02563084,  0.        ],\n",
       "       [ 0.77381781,  0.22887649, -0.73256963, ...,  0.01317788,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.77382644,  0.22815082, -0.77399725, ...,  0.63508411,\n",
       "        -0.01375692,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file = \"/Tmp/linxinle/ASL-data/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
    "# file = \"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/Database-augmented/test/coffee/2sGQuduhAf41354.csv\"\n",
    "file = \"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train/book/41X2t_s2Ai40.csv\"\n",
    "process_csv_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_padding(arr, max_length):\n",
    "    arr = np.append(arr, np.zeros((max_length-arr.shape[0],300)), axis=0)\n",
    "    return np.expand_dims(arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(folder_path, labels, train=True):\n",
    "    total = 0\n",
    "    max_length = 211\n",
    "    \n",
    "    # find max length\n",
    "    # for folder in os.listdir(folder_path):\n",
    "    #     print(folder)\n",
    "    #     if folder != \".DS_Store\":\n",
    "    #         total += len(os.listdir(os.path.join(folder_path, folder)))\n",
    "    #         for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "    #             if file != \".DS_Store\":\n",
    "    #                 data = process_csv_file(os.path.join(folder_path, folder, file))\n",
    "    #                 if data is not(False):\n",
    "    #                     if data.shape[0] > max_length:\n",
    "    #                         max_length = data.shape[0]\n",
    "    # print(max_length)\n",
    "    # Make the arrays\n",
    "    X = np.empty((0, max_length, 300))\n",
    "    Y = np.empty((0,), int)\n",
    "    print(\"----------\")\n",
    "    for folder in os.listdir(folder_path):\n",
    "        print(folder)\n",
    "        if folder != \".DS_Store\":\n",
    "            i=0\n",
    "            for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                i+=1\n",
    "                if file != \".DS_Store\": \n",
    "                    data = process_csv_file(os.path.join(folder_path, folder, file))\n",
    "                    if data is not(False):\n",
    "                        data = set_padding(data, max_length)\n",
    "                        X = np.append(X, data, axis=0)\n",
    "                        Y = np.append(Y, labels[folder])\n",
    "                        if (i% 50 == 0):\n",
    "                            print(i)\n",
    "                if i == 5:\n",
    "                    break\n",
    "                    \n",
    "    print(total, max_length)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hear': 0,\n",
       " 'music': 1,\n",
       " 'bookstore': 2,\n",
       " 'classroom': 3,\n",
       " 'doctor': 4,\n",
       " 'focus': 5,\n",
       " 'chicken': 6,\n",
       " 'door': 7,\n",
       " 'many': 8,\n",
       " 'polite': 9,\n",
       " 'good morning': 10,\n",
       " 'coffee': 11,\n",
       " 'photographer': 12,\n",
       " 'hamburger': 13,\n",
       " 'i': 14,\n",
       " 'phone': 15,\n",
       " 'brother': 16,\n",
       " 'i love you': 17,\n",
       " 'milk': 18,\n",
       " 'dog': 19,\n",
       " 'ocean': 20,\n",
       " 'research': 21,\n",
       " 'book': 22,\n",
       " 'open': 23,\n",
       " 'money': 24}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "def get_labels(folder_path):\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    for folder in os.listdir(folder_path):\n",
    "            labels[folder] = count\n",
    "            count += 1\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(\"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "hear\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "music\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "bookstore\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "950\n",
      "classroom\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "doctor\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "focus\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "chicken\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "door\n",
      "50\n",
      "100\n",
      "150\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "many\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "polite\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "950\n",
      "1000\n",
      "1050\n",
      "good morning\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "coffee\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "photographer\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "hamburger\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "i\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "phone\n",
      "50\n",
      "100\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "brother\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "i love you\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "milk\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "dog\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2550\n",
      "2600\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "ocean\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "folder_path_train = \"/Tmp/shariffa/ASL-words-translator/6_Databases/Aug25LabelsData/train\"\n",
    "# folder_path_train=\"/media/makishima/DATA1/Personnel/Other learning/Programming/Personal_projects/ASL_Language_translation/training_models/mediapipe/data_25_labels_augmentation/train/\"\n",
    "X_train, Y_train = prepare_data(folder_path_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path_test = \"/Tmp/ASL-data/Database-augmented/test\"\n",
    "folder_path_test = \"/Tmp/linxinle/ASL-data/Database-augmented/test\"\n",
    "\n",
    "# labels = {\"coffee\": 0, 'dog': 1, 'milk': 2, 'door': 3}\n",
    "X_test, Y_test = prepare_data(folder_path_test, labels, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 batch_size,\n",
    "                 input_shape=(211, 300),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "        self.n_name = df[y_col['name']].nunique()\n",
    "        self.n_type = df[y_col['type']].nunique()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "], padding=\"post\"\n",
    ")\n",
    "# print(X[0][1])\n",
    "print(((padded_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Y = np.expand_dims(Y, axis=0)\n",
    "# Y= Y.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_val,Y_val = shuffle(X_test, Y_test, random_state=0)\n",
    "X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 211)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " ...\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]], shape=(20, 211), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 18:43:38.595080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.595356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.595543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.999251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.999411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.999529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-19 18:43:38.999632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 332 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 211, 211), dtype=float32, numpy=\n",
       "array([[[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]],\n",
       "\n",
       "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]],\n",
       "\n",
       "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]],\n",
       "\n",
       "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]],\n",
       "\n",
       "       [[ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        ...,\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343],\n",
       "        [ 0.0204925 , -0.02638774, -0.04027992, ...,  0.01537653,\n",
       "         -0.0169578 , -0.02144343]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model - 4 labels\n",
    "from tensorflow.keras import layers\n",
    "x_shape = (457, 211, 300)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(211, 300)))\n",
    "# model_lstm.add(Dropout(0.2))\n",
    "# model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(64))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(25, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model - 25 labels\n",
    "from tensorflow.keras import layers\n",
    "x_shape = (457, 211, 300)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(211, 300)))\n",
    "# model_lstm.add(Dropout(0.2))\n",
    "# model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(64, return_sequences=True))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(0.05)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(128, activation=\"relu\"))\n",
    "model_lstm.add(Dense(64, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(0.05)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(64, activation=\"relu\"))\n",
    "model_lstm.add(Dense(25, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"/Tmp/shariffa/LSTM-25-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "#     \"/Tmp/linxinle/models/LSTM-25-labels.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model_lstm.fit(X_train, Y_train, epochs=50, validation_data=(X_val, Y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl-converter",
   "language": "python",
   "name": "asl-converter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
